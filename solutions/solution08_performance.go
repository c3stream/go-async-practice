package solutions

import (
	"context"
	"fmt"
	"runtime"
	"sync"
	"sync/atomic"
	"time"
)

// Solution08_FixedPerformance - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã®è§£æ±ºç‰ˆ
func Solution08_FixedPerformance() {
	fmt.Println("\nğŸ”§ è§£ç­”8: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã®æœ€é©åŒ–")
	fmt.Println("=" + repeatString("=", 50))

	// è§£æ³•1: ãƒ­ãƒƒã‚¯ç«¶åˆã®æœ€å°åŒ–
	solution1MinimizeLockContention()

	// è§£æ³•2: ãƒãƒ£ãƒãƒ«ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã®æœ€é©åŒ–
	solution2OptimizeChannelBuffering()

	// è§£æ³•3: ã‚´ãƒ«ãƒ¼ãƒãƒ³ãƒ—ãƒ¼ãƒ«ã®æœ€é©åŒ–
	solution3OptimizeGoroutinePool()

	// è§£æ³•4: ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ€é©åŒ–
	solution4OptimizeMemoryAllocation()

	// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
	performanceComparison()
}

// è§£æ³•1: ãƒ­ãƒƒã‚¯ç«¶åˆã®æœ€å°åŒ–
func solution1MinimizeLockContention() {
	fmt.Println("\nğŸ“Œ è§£æ³•1: ãƒ­ãƒƒã‚¯ç«¶åˆã®æœ€å°åŒ–")

	// å•é¡Œã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ãƒƒã‚¯ï¼‰
	type SlowCounter struct {
		mu    sync.Mutex
		value int64
	}

	slowCounter := &SlowCounter{}
	slowIncrement := func() {
		slowCounter.mu.Lock()
		time.Sleep(time.Microsecond) // å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
		slowCounter.value++
		slowCounter.mu.Unlock()
	}

	// æ”¹å–„1: ç´°ç²’åº¦ãƒ­ãƒƒã‚¯ï¼ˆã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
	type ShardedCounter struct {
		shards [16]struct {
			mu    sync.Mutex
			value int64
		}
	}

	shardedCounter := &ShardedCounter{}
	shardedIncrement := func(id int) {
		shard := &shardedCounter.shards[id%16]
		shard.mu.Lock()
		shard.value++
		shard.mu.Unlock()
	}

	shardedTotal := func() int64 {
		var total int64
		for i := range shardedCounter.shards {
			shardedCounter.shards[i].mu.Lock()
			total += shardedCounter.shards[i].value
			shardedCounter.shards[i].mu.Unlock()
		}
		return total
	}

	// æ”¹å–„2: ã‚¢ãƒˆãƒŸãƒƒã‚¯æ“ä½œ
	var atomicCounter int64
	atomicIncrement := func() {
		atomic.AddInt64(&atomicCounter, 1)
	}

	// æ”¹å–„3: ãƒ­ãƒƒã‚¯ãƒ•ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
	type LockFreeQueue struct {
		head atomic.Pointer[node]
		tail atomic.Pointer[node]
	}

	type node struct {
		value int
		next  atomic.Pointer[node]
	}

	queue := &LockFreeQueue{}
	sentinel := &node{}
	queue.head.Store(sentinel)
	queue.tail.Store(sentinel)

	enqueue := func(value int) {
		newNode := &node{value: value}
		for {
			last := queue.tail.Load()
			next := last.next.Load()
			if last == queue.tail.Load() {
				if next == nil {
					if last.next.CompareAndSwap(next, newNode) {
						queue.tail.CompareAndSwap(last, newNode)
						return
					}
				} else {
					queue.tail.CompareAndSwap(last, next)
				}
			}
		}
	}

	// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
	const numOperations = 10000
	numWorkers := runtime.NumCPU()

	// Slow version
	start := time.Now()
	var wg sync.WaitGroup
	for i := 0; i < numWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for j := 0; j < numOperations/numWorkers; j++ {
				slowIncrement()
			}
		}()
	}
	wg.Wait()
	slowTime := time.Since(start)

	// Sharded version
	start = time.Now()
	for i := 0; i < numWorkers; i++ {
		wg.Add(1)
		go func(id int) {
			defer wg.Done()
			for j := 0; j < numOperations/numWorkers; j++ {
				shardedIncrement(id)
			}
		}(i)
	}
	wg.Wait()
	shardedTime := time.Since(start)

	// Atomic version
	start = time.Now()
	for i := 0; i < numWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for j := 0; j < numOperations/numWorkers; j++ {
				atomicIncrement()
			}
		}()
	}
	wg.Wait()
	atomicTime := time.Since(start)

	// çµæœè¡¨ç¤º
	fmt.Printf("\n  ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ (%dæ“ä½œ, %dãƒ¯ãƒ¼ã‚«ãƒ¼):\n", numOperations, numWorkers)
	fmt.Printf("    ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ãƒƒã‚¯: %v\n", slowTime)
	fmt.Printf("    ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: %v (%.2fxé«˜é€Ÿ)\n", shardedTime, float64(slowTime)/float64(shardedTime))
	fmt.Printf("    ã‚¢ãƒˆãƒŸãƒƒã‚¯æ“ä½œ: %v (%.2fxé«˜é€Ÿ)\n", atomicTime, float64(slowTime)/float64(atomicTime))
	fmt.Printf("\n  æ¤œè¨¼:\n")
	fmt.Printf("    Slow Counter: %d\n", slowCounter.value)
	fmt.Printf("    Sharded Counter: %d\n", shardedTotal())
	fmt.Printf("    Atomic Counter: %d\n", atomicCounter)
}

// è§£æ³•2: ãƒãƒ£ãƒãƒ«ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã®æœ€é©åŒ–
func solution2OptimizeChannelBuffering() {
	fmt.Println("\nğŸ“Œ è§£æ³•2: ãƒãƒ£ãƒãƒ«ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã®æœ€é©åŒ–")

	const numItems = 10000

	// éåŠ¹ç‡: ãƒãƒƒãƒ•ã‚¡ãªã—ãƒãƒ£ãƒãƒ«
	benchmarkUnbuffered := func() time.Duration {
		ch := make(chan int)
		done := make(chan bool)

		go func() {
			for range ch {
				// å‡¦ç†
			}
			done <- true
		}()

		start := time.Now()
		for i := 0; i < numItems; i++ {
			ch <- i
		}
		close(ch)
		<-done
		return time.Since(start)
	}

	// æ”¹å–„: é©åˆ‡ãªãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
	benchmarkBuffered := func(bufferSize int) time.Duration {
		ch := make(chan int, bufferSize)
		done := make(chan bool)

		go func() {
			for range ch {
				// å‡¦ç†
			}
			done <- true
		}()

		start := time.Now()
		for i := 0; i < numItems; i++ {
			ch <- i
		}
		close(ch)
		<-done
		return time.Since(start)
	}

	// ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè£…
	type Batch struct {
		items []int
	}

	benchmarkBatched := func(batchSize int) time.Duration {
		ch := make(chan Batch, 10)
		done := make(chan bool)

		go func() {
			for batch := range ch {
				// ãƒãƒƒãƒå‡¦ç†
				_ = batch
			}
			done <- true
		}()

		start := time.Now()
		batch := Batch{items: make([]int, 0, batchSize)}
		for i := 0; i < numItems; i++ {
			batch.items = append(batch.items, i)
			if len(batch.items) == batchSize {
				ch <- batch
				batch = Batch{items: make([]int, 0, batchSize)}
			}
		}
		if len(batch.items) > 0 {
			ch <- batch
		}
		close(ch)
		<-done
		return time.Since(start)
	}

	// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
	fmt.Println("\n  ğŸ“Š ãƒãƒ£ãƒãƒ«ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°æ¯”è¼ƒ:")
	unbufferedTime := benchmarkUnbuffered()
	fmt.Printf("    ãƒãƒƒãƒ•ã‚¡ãªã—: %v\n", unbufferedTime)

	bufferSizes := []int{1, 10, 100, 1000}
	for _, size := range bufferSizes {
		bufferedTime := benchmarkBuffered(size)
		speedup := float64(unbufferedTime) / float64(bufferedTime)
		fmt.Printf("    ãƒãƒƒãƒ•ã‚¡=%d: %v (%.2fxé«˜é€Ÿ)\n", size, bufferedTime, speedup)
	}

	batchSizes := []int{10, 50, 100}
	for _, size := range batchSizes {
		batchedTime := benchmarkBatched(size)
		speedup := float64(unbufferedTime) / float64(batchedTime)
		fmt.Printf("    ãƒãƒƒãƒ=%d: %v (%.2fxé«˜é€Ÿ)\n", size, batchedTime, speedup)
	}

	// select with default ãƒ‘ã‚¿ãƒ¼ãƒ³
	fmt.Println("\n  éãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°é€ä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³:")
	ch := make(chan int, 100)
	sent, dropped := 0, 0

	for i := 0; i < 200; i++ {
		select {
		case ch <- i:
			sent++
		default:
			dropped++
		}
	}

	fmt.Printf("    é€ä¿¡æˆåŠŸ: %d, ãƒ‰ãƒ­ãƒƒãƒ—: %d\n", sent, dropped)
}

// è§£æ³•3: ã‚´ãƒ«ãƒ¼ãƒãƒ³ãƒ—ãƒ¼ãƒ«ã®æœ€é©åŒ–
func solution3OptimizeGoroutinePool() {
	fmt.Println("\nğŸ“Œ è§£æ³•3: ã‚´ãƒ«ãƒ¼ãƒãƒ³ãƒ—ãƒ¼ãƒ«ã®æœ€é©åŒ–")

	// ã‚¿ã‚¹ã‚¯å®šç¾©
	type Task func()

	// éåŠ¹ç‡: ã‚´ãƒ«ãƒ¼ãƒãƒ³ã‚’éƒ½åº¦ä½œæˆ
	runWithoutPool := func(tasks []Task) time.Duration {
		start := time.Now()
		var wg sync.WaitGroup
		for _, task := range tasks {
			wg.Add(1)
			go func(t Task) {
				defer wg.Done()
				t()
			}(task)
		}
		wg.Wait()
		return time.Since(start)
	}

	// æ”¹å–„: ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«
	type WorkerPool struct {
		workers int
		tasks   chan Task
		wg      sync.WaitGroup
	}

	newWorkerPool := func(workers int) *WorkerPool {
		pool := &WorkerPool{
			workers: workers,
			tasks:   make(chan Task, workers*2),
		}

		for i := 0; i < workers; i++ {
			pool.wg.Add(1)
			go func() {
				defer pool.wg.Done()
				for task := range pool.tasks {
					task()
				}
			}()
		}

		return pool
	}

	runWithPool := func(pool *WorkerPool, tasks []Task) time.Duration {
		start := time.Now()
		for _, task := range tasks {
			pool.tasks <- task
		}
		return time.Since(start)
	}

	// ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«
	type DynamicPool struct {
		minWorkers  int
		maxWorkers  int
		tasks       chan Task
		workerCount int32
		mu          sync.Mutex
	}

	newDynamicPool := func(min, max int) *DynamicPool {
		pool := &DynamicPool{
			minWorkers: min,
			maxWorkers: max,
			tasks:      make(chan Task, max*2),
		}

		// æœ€å°ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’èµ·å‹•
		for i := 0; i < min; i++ {
			atomic.AddInt32(&pool.workerCount, 1)
			go pool.worker()
		}

		// è² è·ç›£è¦–
		go pool.monitor()

		return pool
	}

	(func(p *DynamicPool) {
		p.worker = func() {
			for task := range p.tasks {
				task()
			}
			atomic.AddInt32(&p.workerCount, -1)
		}

		p.monitor = func() {
			ticker := time.NewTicker(100 * time.Millisecond)
			defer ticker.Stop()

			for range ticker.C {
				queueSize := len(p.tasks)
				currentWorkers := atomic.LoadInt32(&p.workerCount)

				// ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
				if queueSize > int(currentWorkers)*2 && currentWorkers < int32(p.maxWorkers) {
					atomic.AddInt32(&p.workerCount, 1)
					go p.worker()
				}
				// ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼ˆç°¡ç•¥åŒ–ï¼‰
				// å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚¢ã‚¤ãƒ‰ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ç®¡ç†ãŒå¿…è¦
			}
		}
	})(nil) // ãƒ¡ã‚½ãƒƒãƒ‰å®šç¾©ã®ãŸã‚ã®ãƒ€ãƒŸãƒ¼å‘¼ã³å‡ºã—

	// ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ä½œæˆ
	createTasks := func(count int) []Task {
		tasks := make([]Task, count)
		for i := range tasks {
			tasks[i] = func() {
				time.Sleep(time.Microsecond * 10)
			}
		}
		return tasks
	}

	// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
	numTasks := 1000
	tasks := createTasks(numTasks)

	fmt.Println("\n  ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«æ¯”è¼ƒ:")

	// Without pool
	noPoolTime := runWithoutPool(tasks)
	fmt.Printf("    ãƒ—ãƒ¼ãƒ«ãªã—: %v\n", noPoolTime)

	// With different pool sizes
	poolSizes := []int{4, 8, 16, 32}
	for _, size := range poolSizes {
		pool := newWorkerPool(size)
		poolTime := runWithPool(pool, tasks)
		close(pool.tasks)
		pool.wg.Wait()

		speedup := float64(noPoolTime) / float64(poolTime)
		fmt.Printf("    ãƒ—ãƒ¼ãƒ«(workers=%d): %v (%.2fxé«˜é€Ÿ)\n", size, poolTime, speedup)
	}

	// æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®æ¨å®š
	optimalWorkers := runtime.NumCPU() * 2
	fmt.Printf("\n  ğŸ’¡ æ¨å¥¨ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: %d (CPUæ•°Ã—2)\n", optimalWorkers)
}

// DynamicPool ãƒ¡ã‚½ãƒƒãƒ‰
type DynamicPool struct {
	minWorkers  int
	maxWorkers  int
	tasks       chan func()
	workerCount int32
	mu          sync.Mutex
	worker      func()
	monitor     func()
}

// è§£æ³•4: ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ€é©åŒ–
func solution4OptimizeMemoryAllocation() {
	fmt.Println("\nğŸ“Œ è§£æ³•4: ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ€é©åŒ–")

	const numOperations = 10000

	// éåŠ¹ç‡: é »ç¹ãªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
	inefficientAllocation := func() time.Duration {
		start := time.Now()
		for i := 0; i < numOperations; i++ {
			data := make([]byte, 1024)
			// ãƒ‡ãƒ¼ã‚¿å‡¦ç†
			_ = data
		}
		return time.Since(start)
	}

	// æ”¹å–„1: sync.Pool ã‚’ä½¿ã£ãŸå†åˆ©ç”¨
	bufferPool := sync.Pool{
		New: func() interface{} {
			return make([]byte, 1024)
		},
	}

	pooledAllocation := func() time.Duration {
		start := time.Now()
		for i := 0; i < numOperations; i++ {
			data := bufferPool.Get().([]byte)
			// ãƒ‡ãƒ¼ã‚¿å‡¦ç†
			// ä½¿ç”¨å¾Œãƒ—ãƒ¼ãƒ«ã«æˆ»ã™
			bufferPool.Put(data)
		}
		return time.Since(start)
	}

	// æ”¹å–„2: äº‹å‰å‰²ã‚Šå½“ã¦
	preallocatedSlice := func() time.Duration {
		// å®¹é‡ã‚’äº‹å‰ã«ç¢ºä¿
		results := make([]int, 0, numOperations)

		start := time.Now()
		for i := 0; i < numOperations; i++ {
			results = append(results, i*2)
		}
		return time.Since(start)
	}

	dynamicSlice := func() time.Duration {
		// å®¹é‡æŒ‡å®šãªã—ï¼ˆå‹•çš„æ‹¡å¼µï¼‰
		var results []int

		start := time.Now()
		for i := 0; i < numOperations; i++ {
			results = append(results, i*2)
		}
		return time.Since(start)
	}

	// æ”¹å–„3: ã‚¹ãƒ©ã‚¤ã‚¹ã®å†åˆ©ç”¨
	type DataProcessor struct {
		buffer []byte
		mu     sync.Mutex
	}

	processor := &DataProcessor{
		buffer: make([]byte, 0, 1024),
	}

	reuseSlice := func() time.Duration {
		start := time.Now()
		processor.mu.Lock()
		defer processor.mu.Unlock()

		for i := 0; i < numOperations; i++ {
			// ã‚¹ãƒ©ã‚¤ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å†åˆ©ç”¨
			processor.buffer = processor.buffer[:0]
			processor.buffer = append(processor.buffer, byte(i))
		}
		return time.Since(start)
	}

	// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
	fmt.Println("\n  ğŸ“Š ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ¯”è¼ƒ:")

	inefficientTime := inefficientAllocation()
	fmt.Printf("    é »ç¹ãªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³: %v\n", inefficientTime)

	pooledTime := pooledAllocation()
	fmt.Printf("    sync.Poolä½¿ç”¨: %v (%.2fxé«˜é€Ÿ)\n",
		pooledTime, float64(inefficientTime)/float64(pooledTime))

	dynamicTime := dynamicSlice()
	fmt.Printf("    å‹•çš„ã‚¹ãƒ©ã‚¤ã‚¹: %v\n", dynamicTime)

	preallocTime := preallocatedSlice()
	fmt.Printf("    äº‹å‰å‰²ã‚Šå½“ã¦ã‚¹ãƒ©ã‚¤ã‚¹: %v (%.2fxé«˜é€Ÿ)\n",
		preallocTime, float64(dynamicTime)/float64(preallocTime))

	reuseTime := reuseSlice()
	fmt.Printf("    ã‚¹ãƒ©ã‚¤ã‚¹å†åˆ©ç”¨: %v (%.2fxé«˜é€Ÿ)\n",
		reuseTime, float64(inefficientTime)/float64(reuseTime))

	// GCçµ±è¨ˆ
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	fmt.Printf("\n  ğŸ“Š ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ:\n")
	fmt.Printf("    Alloc: %d KB\n", m.Alloc/1024)
	fmt.Printf("    TotalAlloc: %d KB\n", m.TotalAlloc/1024)
	fmt.Printf("    NumGC: %d\n", m.NumGC)
}

// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
func performanceComparison() {
	fmt.Println("\nğŸ¯ ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ")
	fmt.Println("=" + repeatString("=", 50))

	type BenchResult struct {
		name     string
		duration time.Duration
		ops      int
	}

	results := []BenchResult{}

	// CPUãƒã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯
	cpuBoundTask := func() {
		sum := 0
		for i := 0; i < 1000; i++ {
			sum += i * i
		}
		_ = sum
	}

	// IOãƒã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯
	ioBoundTask := func() {
		time.Sleep(time.Microsecond * 100)
	}

	// ä¸¦åˆ—å‡¦ç†ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
	benchmarkParallel := func(name string, task func(), numTasks int) {
		numWorkers := runtime.NumCPU()
		start := time.Now()

		var wg sync.WaitGroup
		taskChan := make(chan int, numTasks)

		// ãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•
		for w := 0; w < numWorkers; w++ {
			wg.Add(1)
			go func() {
				defer wg.Done()
				for range taskChan {
					task()
				}
			}()
		}

		// ã‚¿ã‚¹ã‚¯æŠ•å…¥
		for i := 0; i < numTasks; i++ {
			taskChan <- i
		}
		close(taskChan)
		wg.Wait()

		duration := time.Since(start)
		results = append(results, BenchResult{
			name:     name,
			duration: duration,
			ops:      numTasks,
		})
	}

	// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
	fmt.Println("\n  å®Ÿè¡Œä¸­...")
	benchmarkParallel("CPU-bound (parallel)", cpuBoundTask, 10000)
	benchmarkParallel("IO-bound (parallel)", ioBoundTask, 100)

	// çµæœè¡¨ç¤º
	fmt.Println("\n  ğŸ“Š çµæœ:")
	for _, r := range results {
		opsPerSec := float64(r.ops) / r.duration.Seconds()
		fmt.Printf("    %s:\n", r.name)
		fmt.Printf("      æ™‚é–“: %v\n", r.duration)
		fmt.Printf("      ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: %.0f ops/sec\n", opsPerSec)
	}

	// æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ
	fmt.Println("\n  ğŸ’¡ æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ:")
	hints := []string{
		"CPUãƒã‚¦ãƒ³ãƒ‰: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° = CPUæ•°",
		"IOãƒã‚¦ãƒ³ãƒ‰: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° > CPUæ•°",
		"ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: sync.Pool ã®æ´»ç”¨",
		"ãƒ­ãƒƒã‚¯ç«¶åˆ: ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯atomic",
		"ãƒãƒ£ãƒãƒ«: é©åˆ‡ãªãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º",
	}

	for _, hint := range hints {
		fmt.Printf("    â€¢ %s\n", hint)
	}
}

// Solution08_OptimizationTechniques - æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯é›†
func Solution08_OptimizationTechniques() {
	fmt.Println("\nğŸ“š ä¸¦è¡Œå‡¦ç†ã®æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯")
	fmt.Println("=" + repeatString("=", 50))

	techniques := []struct {
		name        string
		description string
		example     func()
	}{
		{
			"ãƒãƒƒãƒå‡¦ç†",
			"è¤‡æ•°ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã¾ã¨ã‚ã¦å‡¦ç†",
			exampleBatchProcessing,
		},
		{
			"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
			"ã‚¹ãƒ†ãƒ¼ã‚¸ã”ã¨ã«ä¸¦åˆ—åŒ–",
			examplePipeline,
		},
		{
			"ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆãƒ»ãƒ•ã‚¡ãƒ³ã‚¤ãƒ³",
			"ä½œæ¥­ã‚’åˆ†æ•£ã—ã¦çµæœã‚’é›†ç´„",
			exampleFanOutFanIn,
		},
		{
			"ã‚»ãƒãƒ•ã‚©",
			"åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™",
			exampleSemaphore,
		},
	}

	for i, t := range techniques {
		fmt.Printf("\n  %d. %s\n", i+1, t.name)
		fmt.Printf("     %s\n", t.description)
		t.example()
	}
}

func exampleBatchProcessing() {
	items := make(chan int, 100)
	batches := make(chan []int, 10)

	// ãƒãƒƒãƒä½œæˆ
	go func() {
		batch := make([]int, 0, 10)
		for item := range items {
			batch = append(batch, item)
			if len(batch) == 10 {
				batches <- batch
				batch = make([]int, 0, 10)
			}
		}
		if len(batch) > 0 {
			batches <- batch
		}
		close(batches)
	}()

	// ãƒãƒƒãƒå‡¦ç†
	go func() {
		for batch := range batches {
			fmt.Printf("     ãƒãƒƒãƒå‡¦ç†: %d items\n", len(batch))
		}
	}()

	// ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
	for i := 0; i < 25; i++ {
		items <- i
	}
	close(items)
	time.Sleep(100 * time.Millisecond)
}

func examplePipeline() {
	// ã‚¹ãƒ†ãƒ¼ã‚¸1: ç”Ÿæˆ
	generate := func() <-chan int {
		out := make(chan int)
		go func() {
			for i := 0; i < 5; i++ {
				out <- i
			}
			close(out)
		}()
		return out
	}

	// ã‚¹ãƒ†ãƒ¼ã‚¸2: å¤‰æ›
	square := func(in <-chan int) <-chan int {
		out := make(chan int)
		go func() {
			for n := range in {
				out <- n * n
			}
			close(out)
		}()
		return out
	}

	// ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
	for n := range square(generate()) {
		fmt.Printf("     çµæœ: %d\n", n)
	}
}

func exampleFanOutFanIn() {
	// ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆ
	work := func(id int, jobs <-chan int, results chan<- int) {
		for j := range jobs {
			results <- j * 2
		}
	}

	jobs := make(chan int, 10)
	results := make(chan int, 10)

	// ãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•
	for w := 0; w < 3; w++ {
		go work(w, jobs, results)
	}

	// ã‚¸ãƒ§ãƒ–é€ä¿¡
	go func() {
		for j := 0; j < 5; j++ {
			jobs <- j
		}
		close(jobs)
	}()

	// çµæœé›†ç´„
	for i := 0; i < 5; i++ {
		fmt.Printf("     çµæœ: %d\n", <-results)
	}
}

func exampleSemaphore() {
	sem := make(chan struct{}, 2) // åŒæ™‚å®Ÿè¡Œæ•°2

	var wg sync.WaitGroup
	for i := 0; i < 5; i++ {
		wg.Add(1)
		go func(id int) {
			defer wg.Done()
			sem <- struct{}{}        // ã‚»ãƒãƒ•ã‚©å–å¾—
			defer func() { <-sem }() // ã‚»ãƒãƒ•ã‚©è§£æ”¾

			fmt.Printf("     ã‚¿ã‚¹ã‚¯ %d å®Ÿè¡Œä¸­\n", id)
			time.Sleep(50 * time.Millisecond)
		}(i)
	}
	wg.Wait()
}