package practical

import (
	"context"
	"database/sql"
	"fmt"
	"sync"
	"time"

	_ "github.com/marcboeker/go-duckdb" // DuckDB driver
)

// DuckDBAnalytics - DuckDBã‚’ä½¿ã£ãŸä¸¦åˆ—åˆ†æå‡¦ç†
type DuckDBAnalytics struct {
	db *sql.DB
	mu sync.RWMutex
}

// NewDuckDBAnalytics - DuckDBåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
func NewDuckDBAnalytics() (*DuckDBAnalytics, error) {
	// In-memoryãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨
	db, err := sql.Open("duckdb", ":memory:")
	if err != nil {
		return nil, fmt.Errorf("failed to open duckdb: %w", err)
	}

	analytics := &DuckDBAnalytics{
		db: db,
	}

	// ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
	if err := analytics.initializeTables(); err != nil {
		return nil, err
	}

	return analytics, nil
}

// initializeTables - åˆ†æç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆæœŸåŒ–
func (d *DuckDBAnalytics) initializeTables() error {
	queries := []string{
		// ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆã‚¢ãƒ†ãƒ¼ãƒ–ãƒ«
		`CREATE TABLE IF NOT EXISTS events (
			id INTEGER PRIMARY KEY,
			timestamp TIMESTAMP,
			user_id VARCHAR,
			event_type VARCHAR,
			properties JSON,
			value DOUBLE
		)`,
		// é›†è¨ˆçµæœãƒ†ãƒ¼ãƒ–ãƒ«
		`CREATE TABLE IF NOT EXISTS metrics (
			metric_name VARCHAR,
			timestamp TIMESTAMP,
			dimensions JSON,
			value DOUBLE,
			count INTEGER
		)`,
		// ã‚¿ã‚¤ãƒ ã‚·ãƒªãƒ¼ã‚ºãƒ‡ãƒ¼ã‚¿
		`CREATE TABLE IF NOT EXISTS timeseries (
			metric VARCHAR,
			timestamp TIMESTAMP,
			tags JSON,
			value DOUBLE
		)`,
	}

	for _, query := range queries {
		if _, err := d.db.Exec(query); err != nil {
			return fmt.Errorf("failed to create table: %w", err)
		}
	}

	return nil
}

// StreamingIngestion - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä¸¦åˆ—å–ã‚Šè¾¼ã¿
func (d *DuckDBAnalytics) StreamingIngestion(ctx context.Context) {
	fmt.Println("\nğŸ¦† DuckDB ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åˆ†æãƒ‡ãƒ¢")
	fmt.Println("=" + repeatString("=", 50))

	// ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼
	eventStream := make(chan Event, 1000)

	// ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
	var producerWg sync.WaitGroup
	for i := 0; i < 3; i++ {
		producerWg.Add(1)
		go func(producerID int) {
			defer producerWg.Done()
			d.generateEvents(ctx, producerID, eventStream)
		}(i)
	}

	// ãƒãƒƒãƒæŒ¿å…¥ãƒ¯ãƒ¼ã‚«ãƒ¼
	var consumerWg sync.WaitGroup
	batchSize := 100
	workers := 4

	for i := 0; i < workers; i++ {
		consumerWg.Add(1)
		go func(workerID int) {
			defer consumerWg.Done()
			d.batchInsertWorker(ctx, workerID, eventStream, batchSize)
		}(i)
	}

	// ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆ
	go d.realtimeAggregation(ctx)

	// ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ
	go d.windowAggregation(ctx, 5*time.Second)

	// ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
	go d.displayMetrics(ctx)

	// å®Ÿè¡Œæ™‚é–“
	select {
	case <-ctx.Done():
	case <-time.After(10 * time.Second):
	}

	// ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
	close(eventStream)
	producerWg.Wait()
	consumerWg.Wait()

	// æœ€çµ‚çµæœ
	d.showFinalResults()
}

// Event - ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹é€ 
type Event struct {
	ID        int
	Timestamp time.Time
	UserID    string
	EventType string
	Value     float64
	Properties map[string]interface{}
}

// generateEvents - ã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ
func (d *DuckDBAnalytics) generateEvents(ctx context.Context, producerID int, out chan<- Event) {
	eventTypes := []string{"click", "view", "purchase", "signup"}
	id := producerID * 10000

	ticker := time.NewTicker(100 * time.Millisecond)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			event := Event{
				ID:        id,
				Timestamp: time.Now(),
				UserID:    fmt.Sprintf("user_%d", id%100),
				EventType: eventTypes[id%len(eventTypes)],
				Value:     float64(id%1000) + 0.99,
				Properties: map[string]interface{}{
					"source":   fmt.Sprintf("producer_%d", producerID),
					"session":  fmt.Sprintf("session_%d", id%10),
					"platform": []string{"web", "mobile", "api"}[id%3],
				},
			}
			select {
			case out <- event:
				id++
			default:
				// ãƒãƒƒãƒ•ã‚¡ãƒ•ãƒ«ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
			}
		}
	}
}

// batchInsertWorker - ãƒãƒƒãƒæŒ¿å…¥ãƒ¯ãƒ¼ã‚«ãƒ¼
func (d *DuckDBAnalytics) batchInsertWorker(ctx context.Context, workerID int, events <-chan Event, batchSize int) {
	batch := make([]Event, 0, batchSize)
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	flush := func() {
		if len(batch) == 0 {
			return
		}

		tx, err := d.db.Begin()
		if err != nil {
			fmt.Printf("  Worker %d: ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: %v\n", workerID, err)
			return
		}
		defer tx.Rollback()

		stmt, err := tx.Prepare(`
			INSERT INTO events (id, timestamp, user_id, event_type, properties, value)
			VALUES (?, ?, ?, ?, ?, ?)
		`)
		if err != nil {
			fmt.Printf("  Worker %d: ãƒ—ãƒªãƒšã‚¢ã‚¨ãƒ©ãƒ¼: %v\n", workerID, err)
			return
		}
		defer stmt.Close()

		for _, event := range batch {
			properties := fmt.Sprintf(`{"source": "%s", "platform": "%s"}`,
				event.Properties["source"], event.Properties["platform"])

			_, err := stmt.Exec(
				event.ID,
				event.Timestamp,
				event.UserID,
				event.EventType,
				properties,
				event.Value,
			)
			if err != nil {
				fmt.Printf("  Worker %d: æŒ¿å…¥ã‚¨ãƒ©ãƒ¼: %v\n", workerID, err)
			}
		}

		if err := tx.Commit(); err != nil {
			fmt.Printf("  Worker %d: ã‚³ãƒŸãƒƒãƒˆã‚¨ãƒ©ãƒ¼: %v\n", workerID, err)
		} else {
			fmt.Printf("  Worker %d: %dä»¶æŒ¿å…¥å®Œäº†\n", workerID, len(batch))
		}

		batch = batch[:0]
	}

	for {
		select {
		case <-ctx.Done():
			flush()
			return
		case event, ok := <-events:
			if !ok {
				flush()
				return
			}
			batch = append(batch, event)
			if len(batch) >= batchSize {
				flush()
			}
		case <-ticker.C:
			flush()
		}
	}
}

// realtimeAggregation - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆ
func (d *DuckDBAnalytics) realtimeAggregation(ctx context.Context) {
	ticker := time.NewTicker(2 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			// ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥ã®é›†è¨ˆ
			query := `
				SELECT
					event_type,
					COUNT(*) as count,
					SUM(value) as total_value,
					AVG(value) as avg_value,
					MAX(value) as max_value
				FROM events
				WHERE timestamp > NOW() - INTERVAL 10 SECOND
				GROUP BY event_type
			`

			rows, err := d.db.Query(query)
			if err != nil {
				continue
			}

			fmt.Println("\nğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆï¼ˆéå»10ç§’ï¼‰:")
			for rows.Next() {
				var eventType string
				var count int
				var totalValue, avgValue, maxValue float64

				rows.Scan(&eventType, &count, &totalValue, &avgValue, &maxValue)
				fmt.Printf("  %s: ä»¶æ•°=%d, åˆè¨ˆ=%.2f, å¹³å‡=%.2f, æœ€å¤§=%.2f\n",
					eventType, count, totalValue, avgValue, maxValue)
			}
			rows.Close()
		}
	}
}

// windowAggregation - ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ
func (d *DuckDBAnalytics) windowAggregation(ctx context.Context, window time.Duration) {
	ticker := time.NewTicker(window)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			// ç§»å‹•å¹³å‡ã®è¨ˆç®—
			query := `
				SELECT
					event_type,
					DATE_TRUNC('second', timestamp) as time_bucket,
					COUNT(*) OVER (
						PARTITION BY event_type
						ORDER BY DATE_TRUNC('second', timestamp)
						RANGE BETWEEN INTERVAL 5 SECOND PRECEDING AND CURRENT ROW
					) as moving_count,
					AVG(value) OVER (
						PARTITION BY event_type
						ORDER BY DATE_TRUNC('second', timestamp)
						RANGE BETWEEN INTERVAL 5 SECOND PRECEDING AND CURRENT ROW
					) as moving_avg
				FROM events
				WHERE timestamp > NOW() - INTERVAL 30 SECOND
				ORDER BY time_bucket DESC
				LIMIT 10
			`

			rows, err := d.db.Query(query)
			if err != nil {
				continue
			}

			fmt.Println("\nğŸ“ˆ ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆï¼ˆ5ç§’ç§»å‹•å¹³å‡ï¼‰:")
			for rows.Next() {
				var eventType string
				var timeBucket time.Time
				var movingCount int
				var movingAvg float64

				rows.Scan(&eventType, &timeBucket, &movingCount, &movingAvg)
				fmt.Printf("  [%s] %s: ç§»å‹•ä»¶æ•°=%d, ç§»å‹•å¹³å‡=%.2f\n",
					timeBucket.Format("15:04:05"), eventType, movingCount, movingAvg)
			}
			rows.Close()
		}
	}
}

// displayMetrics - ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
func (d *DuckDBAnalytics) displayMetrics(ctx context.Context) {
	ticker := time.NewTicker(3 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			// ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ
			var totalEvents int
			d.db.QueryRow("SELECT COUNT(*) FROM events").Scan(&totalEvents)

			// ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
			var uniqueUsers int
			d.db.QueryRow("SELECT COUNT(DISTINCT user_id) FROM events").Scan(&uniqueUsers)

			// ãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒ¼ãƒˆ
			var eventsPerSecond float64
			d.db.QueryRow(`
				SELECT COUNT(*) * 1.0 /
					EXTRACT(EPOCH FROM (MAX(timestamp) - MIN(timestamp)))
				FROM events
				WHERE timestamp > NOW() - INTERVAL 10 SECOND
			`).Scan(&eventsPerSecond)

			fmt.Printf("\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹: ç·ã‚¤ãƒ™ãƒ³ãƒˆ=%d, ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼=%d, ãƒ¬ãƒ¼ãƒˆ=%.1f/ç§’\n",
				totalEvents, uniqueUsers, eventsPerSecond)
		}
	}
}

// showFinalResults - æœ€çµ‚çµæœè¡¨ç¤º
func (d *DuckDBAnalytics) showFinalResults() {
	fmt.Println("\nğŸ æœ€çµ‚åˆ†æçµæœ:")
	fmt.Println("=" + repeatString("=", 50))

	// Top Nåˆ†æ
	query := `
		WITH user_stats AS (
			SELECT
				user_id,
				COUNT(*) as event_count,
				SUM(value) as total_value,
				COUNT(DISTINCT event_type) as unique_events
			FROM events
			GROUP BY user_id
		)
		SELECT * FROM user_stats
		ORDER BY total_value DESC
		LIMIT 5
	`

	rows, err := d.db.Query(query)
	if err != nil {
		fmt.Printf("ã‚¯ã‚¨ãƒªã‚¨ãƒ©ãƒ¼: %v\n", err)
		return
	}
	defer rows.Close()

	fmt.Println("\nğŸ† Top 5 ãƒ¦ãƒ¼ã‚¶ãƒ¼:")
	rank := 1
	for rows.Next() {
		var userID string
		var eventCount, uniqueEvents int
		var totalValue float64

		rows.Scan(&userID, &eventCount, &totalValue, &uniqueEvents)
		fmt.Printf("  %d. %s: ã‚¤ãƒ™ãƒ³ãƒˆæ•°=%d, åˆè¨ˆå€¤=%.2f, ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¤ãƒ™ãƒ³ãƒˆ=%d\n",
			rank, userID, eventCount, totalValue, uniqueEvents)
		rank++
	}

	// ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«åˆ†æ
	fmt.Println("\nğŸ“Š å€¤ã®åˆ†å¸ƒï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰:")
	percentiles := []float64{0.25, 0.5, 0.75, 0.95, 0.99}
	for _, p := range percentiles {
		var value float64
		query := fmt.Sprintf(`
			SELECT PERCENTILE_CONT(%.2f) WITHIN GROUP (ORDER BY value)
			FROM events
		`, p)
		d.db.QueryRow(query).Scan(&value)
		fmt.Printf("  P%d: %.2f\n", int(p*100), value)
	}
}

// ParallelOLAPQueries - ä¸¦åˆ—OLAPåˆ†æ
func (d *DuckDBAnalytics) ParallelOLAPQueries(ctx context.Context) {
	fmt.Println("\nğŸ”„ ä¸¦åˆ—OLAPåˆ†æãƒ‡ãƒ¢")
	fmt.Println("=" + repeatString("=", 50))

	// ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
	d.generateSampleData()

	queries := []struct {
		name  string
		query string
	}{
		{
			name: "æ—¥æ¬¡é›†è¨ˆ",
			query: `
				SELECT
					DATE_TRUNC('day', timestamp) as day,
					COUNT(*) as daily_events,
					SUM(value) as daily_value
				FROM events
				GROUP BY day
				ORDER BY day
			`,
		},
		{
			name: "ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥åˆ†æ",
			query: `
				SELECT
					event_type,
					COUNT(*) as count,
					AVG(value) as avg_value,
					STDDEV(value) as stddev_value
				FROM events
				GROUP BY event_type
			`,
		},
		{
			name: "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ",
			query: `
				WITH user_segments AS (
					SELECT
						user_id,
						CASE
							WHEN SUM(value) > 1000 THEN 'High'
							WHEN SUM(value) > 500 THEN 'Medium'
							ELSE 'Low'
						END as segment
					FROM events
					GROUP BY user_id
				)
				SELECT segment, COUNT(*) as user_count
				FROM user_segments
				GROUP BY segment
			`,
		},
		{
			name: "æ™‚é–“å¸¯åˆ†æ",
			query: `
				SELECT
					EXTRACT(HOUR FROM timestamp) as hour,
					COUNT(*) as event_count,
					AVG(value) as avg_value
				FROM events
				GROUP BY hour
				ORDER BY hour
			`,
		},
	}

	// ä¸¦åˆ—ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
	var wg sync.WaitGroup
	results := make(chan QueryResult, len(queries))

	for _, q := range queries {
		wg.Add(1)
		go func(name, query string) {
			defer wg.Done()

			start := time.Now()
			rows, err := d.db.Query(query)
			elapsed := time.Since(start)

			if err != nil {
				results <- QueryResult{
					Name:    name,
					Error:   err,
					Elapsed: elapsed,
				}
				return
			}
			defer rows.Close()

			// çµæœã‚’åé›†
			var resultData [][]interface{}
			cols, _ := rows.Columns()

			for rows.Next() {
				values := make([]interface{}, len(cols))
				valuePtrs := make([]interface{}, len(cols))
				for i := range values {
					valuePtrs[i] = &values[i]
				}
				rows.Scan(valuePtrs...)
				resultData = append(resultData, values)
			}

			results <- QueryResult{
				Name:    name,
				Data:    resultData,
				Elapsed: elapsed,
			}
		}(q.name, q.query)
	}

	// çµæœåé›†
	go func() {
		wg.Wait()
		close(results)
	}()

	// çµæœè¡¨ç¤º
	for result := range results {
		if result.Error != nil {
			fmt.Printf("\nâŒ %s: ã‚¨ãƒ©ãƒ¼ %v\n", result.Name, result.Error)
		} else {
			fmt.Printf("\nâœ… %s (å®Ÿè¡Œæ™‚é–“: %v):\n", result.Name, result.Elapsed)
			for i, row := range result.Data {
				if i < 5 { // æœ€åˆã®5è¡Œã®ã¿è¡¨ç¤º
					fmt.Printf("  %v\n", row)
				}
			}
			if len(result.Data) > 5 {
				fmt.Printf("  ... ä»– %d è¡Œ\n", len(result.Data)-5)
			}
		}
	}
}

// QueryResult - ã‚¯ã‚¨ãƒªçµæœ
type QueryResult struct {
	Name    string
	Data    [][]interface{}
	Error   error
	Elapsed time.Duration
}

// generateSampleData - ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
func (d *DuckDBAnalytics) generateSampleData() {
	// ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
	tx, _ := d.db.Begin()
	stmt, _ := tx.Prepare(`
		INSERT INTO events (id, timestamp, user_id, event_type, properties, value)
		VALUES (?, ?, ?, ?, ?, ?)
	`)

	eventTypes := []string{"click", "view", "purchase", "signup", "logout"}
	baseTime := time.Now().Add(-24 * time.Hour)

	for i := 0; i < 10000; i++ {
		timestamp := baseTime.Add(time.Duration(i) * time.Minute)
		userID := fmt.Sprintf("user_%d", i%100)
		eventType := eventTypes[i%len(eventTypes)]
		value := float64(i%1000) + float64(i%100)/100

		stmt.Exec(i, timestamp, userID, eventType, "{}", value)
	}

	tx.Commit()
	fmt.Println("  âœ“ 10,000ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
}

// Close - ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
func (d *DuckDBAnalytics) Close() error {
	return d.db.Close()
}