package practical

import (
	"context"
	"fmt"
	"sync"
	"sync/atomic"
	"time"

	"github.com/gocql/gocql"
)

// CassandraNoSQL - Cassandraã‚’ä½¿ã£ãŸåˆ†æ•£NoSQLå‡¦ç†
type CassandraNoSQL struct {
	cluster *gocql.ClusterConfig
	session *gocql.Session
	mu      sync.RWMutex
}

// NewCassandraNoSQL - Cassandraæ¥ç¶šã®åˆæœŸåŒ–
func NewCassandraNoSQL(hosts []string) (*CassandraNoSQL, error) {
	cluster := gocql.NewCluster(hosts...)
	cluster.Keyspace = "async_practice"
	cluster.Consistency = gocql.Quorum
	cluster.ProtoVersion = 4
	cluster.Timeout = 5 * time.Second
	cluster.ConnectTimeout = 5 * time.Second

	// æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®è¨­å®š
	cluster.NumConns = 3
	cluster.PoolConfig.HostSelectionPolicy = gocql.TokenAwareHostPolicy(gocql.RoundRobinHostPolicy())

	session, err := cluster.CreateSession()
	if err != nil {
		// ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®CassandraãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰
		fmt.Println("âš  Cassandraã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
		return &CassandraNoSQL{
			cluster: cluster,
			session: nil, // ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§ã¯nilã‚»ãƒƒã‚·ãƒ§ãƒ³
		}, nil
	}

	c := &CassandraNoSQL{
		cluster: cluster,
		session: session,
	}

	// ã‚­ãƒ¼ã‚¹ãƒšãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ
	if err := c.initializeSchema(); err != nil {
		fmt.Printf("ã‚¹ã‚­ãƒ¼ãƒåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: %v\n", err)
	}

	return c, nil
}

// initializeSchema - ã‚¹ã‚­ãƒ¼ãƒã®åˆæœŸåŒ–
func (c *CassandraNoSQL) initializeSchema() error {
	if c.session == nil {
		return nil // ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰
	}

	queries := []string{
		// ã‚­ãƒ¼ã‚¹ãƒšãƒ¼ã‚¹ä½œæˆ
		`CREATE KEYSPACE IF NOT EXISTS async_practice
		 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3}`,

		// ã‚¿ã‚¤ãƒ ã‚·ãƒªãƒ¼ã‚ºãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
		`CREATE TABLE IF NOT EXISTS async_practice.timeseries (
			partition_key text,
			timestamp timestamp,
			sensor_id text,
			value double,
			metadata map<text, text>,
			PRIMARY KEY (partition_key, timestamp, sensor_id)
		) WITH CLUSTERING ORDER BY (timestamp DESC)`,

		// ãƒ¯ã‚¤ãƒ‰ã‚«ãƒ©ãƒ ãƒ†ãƒ¼ãƒ–ãƒ«
		`CREATE TABLE IF NOT EXISTS async_practice.wide_rows (
			row_key text,
			column_name text,
			column_value blob,
			timestamp timestamp,
			PRIMARY KEY (row_key, column_name)
		)`,

		// ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«
		`CREATE TABLE IF NOT EXISTS async_practice.counters (
			counter_id text PRIMARY KEY,
			count counter
		)`,

		// ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼
		`CREATE MATERIALIZED VIEW IF NOT EXISTS async_practice.timeseries_by_sensor AS
			SELECT * FROM async_practice.timeseries
			WHERE sensor_id IS NOT NULL AND partition_key IS NOT NULL AND timestamp IS NOT NULL
			PRIMARY KEY (sensor_id, timestamp, partition_key)
			WITH CLUSTERING ORDER BY (timestamp DESC)`,
	}

	for _, query := range queries {
		if err := c.session.Query(query).Exec(); err != nil {
			// ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ï¼ˆæ—¢å­˜ã®å ´åˆãªã©ï¼‰
			fmt.Printf("ã‚¹ã‚­ãƒ¼ãƒã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: %v\n", err)
		}
	}

	return nil
}

// AdvancedTimeSeriesPatterns - é«˜åº¦ãªæ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³
func (c *CassandraNoSQL) AdvancedTimeSeriesPatterns(ctx context.Context) {
	fmt.Println("\nğŸ• Cassandra é«˜åº¦ãªæ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³")
	fmt.Println("=" + repeatString("=", 50))

	if c.session == nil {
		c.runAdvancedDemoMode(ctx)
		return
	}

	// å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
	c.createAdvancedSchemas()

	// 1. ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ
	go c.rollingWindowAggregation(ctx)

	// 2. æ™‚ç³»åˆ—ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
	go c.timeSeriesDownsampling(ctx)

	// 3. ç•°å¸¸æ¤œçŸ¥
	go c.anomalyDetection(ctx)

	// 4. ãƒ‡ãƒ¼ã‚¿è€æœ½åŒ–ï¼ˆTTLï¼‰ç®¡ç†
	go c.dataLifecycleManagement(ctx)

	select {
	case <-ctx.Done():
	case <-time.After(20 * time.Second):
	}
}

// createAdvancedSchemas - é«˜åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ç”¨ã®ã‚¹ã‚­ãƒ¼ãƒä½œæˆ
func (c *CassandraNoSQL) createAdvancedSchemas() {
	schemas := []string{
		// ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœä¿å­˜ç”¨
		`CREATE TABLE IF NOT EXISTS async_practice.downsampled_data (
			bucket_time timestamp,
			interval_type text,
			sensor_id text,
			avg_value double,
			min_value double,
			max_value double,
			count bigint,
			PRIMARY KEY ((interval_type, sensor_id), bucket_time)
		) WITH CLUSTERING ORDER BY (bucket_time DESC)`,

		// ç•°å¸¸æ¤œçŸ¥çµæœä¿å­˜ç”¨
		`CREATE TABLE IF NOT EXISTS async_practice.anomalies (
			detection_time timestamp,
			sensor_id text,
			anomaly_type text,
			value double,
			threshold double,
			severity text,
			PRIMARY KEY (sensor_id, detection_time)
		) WITH CLUSTERING ORDER BY (detection_time DESC)`,
	}

	for _, schema := range schemas {
		c.session.Query(schema).Exec()
	}
}

// rollingWindowAggregation - ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ
func (c *CassandraNoSQL) rollingWindowAggregation(ctx context.Context) {
	fmt.Println("\nğŸ“Š ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ")

	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			now := time.Now()
			windowEnd := now.Truncate(5 * time.Minute)
			windowStart := windowEnd.Add(-5 * time.Minute)

			var wg sync.WaitGroup
			aggregations := []struct {
				name  string
				query string
			}{
				{
					name: "å¹³å‡å€¤",
					query: fmt.Sprintf(`
						SELECT sensor_id, AVG(value) as avg_value, COUNT(*) as count
						FROM async_practice.timeseries
						WHERE partition_key = 'sensors'
						AND timestamp >= '%s'
						AND timestamp < '%s'
						GROUP BY sensor_id
						ALLOW FILTERING`,
						windowStart.Format(time.RFC3339),
						windowEnd.Format(time.RFC3339)),
				},
			}

			fmt.Printf("  ğŸ” ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ %s - %s:\n",
				windowStart.Format("15:04:05"), windowEnd.Format("15:04:05"))

			for _, agg := range aggregations {
				wg.Add(1)
				go func(name, query string) {
					defer wg.Done()
					iter := c.session.Query(query).Iter()
					count := 0
					m := make(map[string]interface{})

					for iter.MapScan(m) {
						count++
						m = make(map[string]interface{})
					}

					if err := iter.Close(); err == nil {
						fmt.Printf("    %s: %d ã‚»ãƒ³ã‚µãƒ¼ã®çµæœ\n", name, count)
					}
				}(agg.name, agg.query)
			}
			wg.Wait()
		}
	}
}

// timeSeriesDownsampling - æ™‚ç³»åˆ—ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
func (c *CassandraNoSQL) timeSeriesDownsampling(ctx context.Context) {
	fmt.Println("\nâ¬‡ï¸ æ™‚ç³»åˆ—ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")

	ticker := time.NewTicker(45 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			intervals := []struct {
				name     string
				duration time.Duration
			}{
				{"1åˆ†é–“éš”", 1 * time.Minute},
				{"5åˆ†é–“éš”", 5 * time.Minute},
			}

			var wg sync.WaitGroup
			for _, interval := range intervals {
				wg.Add(1)
				go func(name string, duration time.Duration) {
					defer wg.Done()

					now := time.Now()
					bucketEnd := now.Truncate(duration)
					bucketStart := bucketEnd.Add(-duration)

					query := fmt.Sprintf(`
						SELECT sensor_id, COUNT(*) as count
						FROM async_practice.timeseries
						WHERE partition_key = 'sensors'
						AND timestamp >= '%s'
						AND timestamp < '%s'
						GROUP BY sensor_id
						ALLOW FILTERING`,
						bucketStart.Format(time.RFC3339),
						bucketEnd.Format(time.RFC3339))

					iter := c.session.Query(query).Iter()
					count := 0
					m := make(map[string]interface{})

					for iter.MapScan(m) {
						count++
						m = make(map[string]interface{})
					}

					if err := iter.Close(); err == nil {
						fmt.Printf("  ğŸ“‰ %s: %d ã‚»ãƒ³ã‚µãƒ¼ã‚’é›†ç´„\n", name, count)
					}
				}(interval.name, interval.duration)
			}
			wg.Wait()
		}
	}
}

// anomalyDetection - ç•°å¸¸æ¤œçŸ¥
func (c *CassandraNoSQL) anomalyDetection(ctx context.Context) {
	fmt.Println("\nğŸš¨ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç•°å¸¸æ¤œçŸ¥")

	ticker := time.NewTicker(20 * time.Second)
	defer ticker.Stop()

	thresholds := map[string]float64{
		"sensor_0": 500.0,
		"sensor_1": 450.0,
		"sensor_2": 600.0,
	}

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			var wg sync.WaitGroup

			for sensorID, threshold := range thresholds {
				wg.Add(1)
				go func(sensor string, limit float64) {
					defer wg.Done()

					query := `
						SELECT timestamp, value
						FROM async_practice.timeseries_by_sensor
						WHERE sensor_id = ?
						ORDER BY timestamp DESC
						LIMIT 10`

					iter := c.session.Query(query, sensor).Iter()
					var values []float64
					var timestamp time.Time
					var value float64

					for iter.Scan(&timestamp, &value) {
						values = append(values, value)
					}

					if err := iter.Close(); err == nil && len(values) > 0 {
						latestValue := values[0]
						if latestValue > limit {
							fmt.Printf("  ğŸš¨ %s: é–¾å€¤è¶…é %.2f > %.2f\n", sensor, latestValue, limit)
						} else {
							fmt.Printf("  âœ… %s: æ­£å¸¸ %.2f\n", sensor, latestValue)
						}
					}
				}(sensorID, threshold)
			}
			wg.Wait()
		}
	}
}

// dataLifecycleManagement - ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†
func (c *CassandraNoSQL) dataLifecycleManagement(ctx context.Context) {
	fmt.Println("\nğŸ—‚ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†")

	ticker := time.NewTicker(60 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			fmt.Println("  ğŸ“Š ãƒ‡ãƒ¼ã‚¿è€æœ½åŒ–åˆ†æ:")

			cutoffTime := time.Now().Add(-24 * time.Hour)
			query := `
				SELECT COUNT(*) as old_count
				FROM async_practice.timeseries
				WHERE partition_key = 'sensors'
				AND timestamp < ?
				ALLOW FILTERING`

			var oldCount int64
			if err := c.session.Query(query, cutoffTime).Scan(&oldCount); err == nil {
				fmt.Printf("    24æ™‚é–“ä»¥ä¸Šå‰ã®ãƒ‡ãƒ¼ã‚¿: %dä»¶\n", oldCount)
			}

			fmt.Println("    TTLè¨­å®šã«ã‚ˆã‚Šè‡ªå‹•å‰Šé™¤ã•ã‚Œã‚‹äºˆå®šã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª")
		}
	}
}

// runAdvancedDemoMode - é«˜åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰
func (c *CassandraNoSQL) runAdvancedDemoMode(ctx context.Context) {
	fmt.Println("  âš  Cassandraæœªæ¥ç¶š: é«˜åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰")

	demoPatterns := []string{
		"ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ (5åˆ†é–“éš”)",
		"æ™‚ç³»åˆ—ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (1åˆ†â†’5åˆ†)",
		"ç•°å¸¸æ¤œçŸ¥ (é–¾å€¤ãƒ™ãƒ¼ã‚¹)",
		"ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç† (TTL + åˆ†æ)",
	}

	for i, pattern := range demoPatterns {
		time.Sleep(2 * time.Second)
		fmt.Printf("  ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³%d: %s - âœ… ãƒ‡ãƒ¢å®Œäº†\n", i+1, pattern)
	}

	fmt.Println("  âœ… é«˜åº¦æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¢å®Œäº†")
}

// TimeSeriesIngestion - ã‚¿ã‚¤ãƒ ã‚·ãƒªãƒ¼ã‚ºãƒ‡ãƒ¼ã‚¿ã®ä¸¦åˆ—å–ã‚Šè¾¼ã¿
func (c *CassandraNoSQL) TimeSeriesIngestion(ctx context.Context) {
	fmt.Println("\nğŸ—„ Cassandra ã‚¿ã‚¤ãƒ ã‚·ãƒªãƒ¼ã‚ºå–ã‚Šè¾¼ã¿ãƒ‡ãƒ¢")
	fmt.Println("=" + repeatString("=", 50))

	if c.session == nil {
		c.runDemoMode(ctx)
		return
	}

	// ãƒ¡ãƒˆãƒªã‚¯ã‚¹
	var (
		totalInserted  int64
		totalFailed    int64
		totalBatches   int64
	)

	// ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼
	dataStream := make(chan SensorData, 1000)

	// ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆä¸¦åˆ—ï¼‰
	var producerWg sync.WaitGroup
	numSensors := 10
	for i := 0; i < numSensors; i++ {
		producerWg.Add(1)
		go func(sensorID int) {
			defer producerWg.Done()
			c.generateSensorData(ctx, sensorID, dataStream)
		}(i)
	}

	// ãƒãƒƒãƒæ›¸ãè¾¼ã¿ãƒ¯ãƒ¼ã‚«ãƒ¼
	var writerWg sync.WaitGroup
	numWriters := 5
	batchSize := 50

	for i := 0; i < numWriters; i++ {
		writerWg.Add(1)
		go func(writerID int) {
			defer writerWg.Done()
			c.batchWriter(ctx, writerID, dataStream, batchSize,
				&totalInserted, &totalFailed, &totalBatches)
		}(i)
	}

	// çµ±è¨ˆè¡¨ç¤º
	go c.displayStats(ctx, &totalInserted, &totalFailed, &totalBatches)

	// ä¸¦åˆ—èª­ã¿å–ã‚Šãƒ‡ãƒ¢
	go c.parallelReads(ctx)

	// å®Ÿè¡Œ
	select {
	case <-ctx.Done():
	case <-time.After(15 * time.Second):
	}

	// ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
	close(dataStream)
	producerWg.Wait()
	writerWg.Wait()

	fmt.Printf("\nğŸ“Š æœ€çµ‚çµ±è¨ˆ: æŒ¿å…¥=%d, å¤±æ•—=%d, ãƒãƒƒãƒ=%d\n",
		atomic.LoadInt64(&totalInserted),
		atomic.LoadInt64(&totalFailed),
		atomic.LoadInt64(&totalBatches))
}

// SensorData - ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿
type SensorData struct {
	SensorID    string
	Timestamp   time.Time
	Value       float64
	Metadata    map[string]string
}

// generateSensorData - ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
func (c *CassandraNoSQL) generateSensorData(ctx context.Context, sensorID int, out chan<- SensorData) {
	ticker := time.NewTicker(100 * time.Millisecond)
	defer ticker.Stop()

	sensorName := fmt.Sprintf("sensor_%d", sensorID)
	location := []string{"tokyo", "osaka", "nagoya"}[sensorID%3]

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			data := SensorData{
				SensorID:  sensorName,
				Timestamp: time.Now(),
				Value:     float64(sensorID*100) + float64(time.Now().Unix()%100),
				Metadata: map[string]string{
					"location": location,
					"type":     "temperature",
					"unit":     "celsius",
				},
			}

			select {
			case out <- data:
			default:
				// ãƒãƒƒãƒ•ã‚¡ãƒ•ãƒ«ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
			}
		}
	}
}

// batchWriter - ãƒãƒƒãƒæ›¸ãè¾¼ã¿ãƒ¯ãƒ¼ã‚«ãƒ¼
func (c *CassandraNoSQL) batchWriter(ctx context.Context, writerID int,
	dataStream <-chan SensorData, batchSize int,
	totalInserted, totalFailed, totalBatches *int64) {

	batch := make([]SensorData, 0, batchSize)
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	flush := func() {
		if len(batch) == 0 {
			return
		}

		// ãƒãƒƒãƒã‚¯ã‚¨ãƒªã®æº–å‚™
		batchQuery := c.session.NewBatch(gocql.LoggedBatch)
		batchQuery.SetConsistency(gocql.LocalQuorum)

		for _, data := range batch {
			// ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã®ç”Ÿæˆï¼ˆæ—¥ä»˜ãƒ™ãƒ¼ã‚¹ï¼‰
			partitionKey := fmt.Sprintf("%s_%s",
				data.SensorID,
				data.Timestamp.Format("2006-01-02"))

			batchQuery.Query(`
				INSERT INTO async_practice.timeseries
				(partition_key, timestamp, sensor_id, value, metadata)
				VALUES (?, ?, ?, ?, ?)`,
				partitionKey,
				data.Timestamp,
				data.SensorID,
				data.Value,
				data.Metadata,
			)
		}

		// ãƒãƒƒãƒå®Ÿè¡Œï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
		var err error
		for retry := 0; retry < 3; retry++ {
			err = c.session.ExecuteBatch(batchQuery)
			if err == nil {
				break
			}
			time.Sleep(time.Duration(retry*100) * time.Millisecond)
		}

		if err != nil {
			atomic.AddInt64(totalFailed, int64(len(batch)))
			fmt.Printf("  Writer %d: ãƒãƒƒãƒæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: %v\n", writerID, err)
		} else {
			atomic.AddInt64(totalInserted, int64(len(batch)))
			atomic.AddInt64(totalBatches, 1)
		}

		batch = batch[:0]
	}

	for {
		select {
		case <-ctx.Done():
			flush()
			return
		case data, ok := <-dataStream:
			if !ok {
				flush()
				return
			}
			batch = append(batch, data)
			if len(batch) >= batchSize {
				flush()
			}
		case <-ticker.C:
			flush()
		}
	}
}

// parallelReads - ä¸¦åˆ—èª­ã¿å–ã‚Šãƒ‡ãƒ¢
func (c *CassandraNoSQL) parallelReads(ctx context.Context) {
	if c.session == nil {
		return // ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰
	}

	ticker := time.NewTicker(3 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			// ä¸¦åˆ—ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
			var wg sync.WaitGroup
			queries := []struct {
				name  string
				query string
			}{
				{
					name: "æœ€æ–°ãƒ‡ãƒ¼ã‚¿",
					query: `SELECT sensor_id, timestamp, value
							FROM async_practice.timeseries
							LIMIT 10`,
				},
				{
					name: "ã‚»ãƒ³ã‚µãƒ¼åˆ¥é›†è¨ˆ",
					query: `SELECT sensor_id, COUNT(*) as count
							FROM async_practice.timeseries
							GROUP BY sensor_id
							ALLOW FILTERING`,
				},
			}

			fmt.Println("\nğŸ“– ä¸¦åˆ—èª­ã¿å–ã‚Šçµæœ:")
			for _, q := range queries {
				wg.Add(1)
				go func(name, query string) {
					defer wg.Done()

					iter := c.session.Query(query).Iter()

					var results []map[string]interface{}
					m := make(map[string]interface{})
					for iter.MapScan(m) {
						results = append(results, m)
						m = make(map[string]interface{})
					}

					if err := iter.Close(); err != nil {
						fmt.Printf("  %s: ã‚¨ãƒ©ãƒ¼ %v\n", name, err)
					} else {
						fmt.Printf("  %s: %dä»¶å–å¾—\n", name, len(results))
					}
				}(q.name, q.query)
			}
			wg.Wait()
		}
	}
}

// displayStats - çµ±è¨ˆè¡¨ç¤º
func (c *CassandraNoSQL) displayStats(ctx context.Context,
	totalInserted, totalFailed, totalBatches *int64) {

	ticker := time.NewTicker(2 * time.Second)
	defer ticker.Stop()

	var lastInserted int64
	startTime := time.Now()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			inserted := atomic.LoadInt64(totalInserted)
			failed := atomic.LoadInt64(totalFailed)
			batches := atomic.LoadInt64(totalBatches)

			// ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
			throughput := float64(inserted-lastInserted) / 2.0 // per second
			elapsed := time.Since(startTime).Seconds()
			avgThroughput := float64(inserted) / elapsed

			fmt.Printf("\nğŸ“Š çµ±è¨ˆ: æŒ¿å…¥=%d, å¤±æ•—=%d, ãƒãƒƒãƒ=%d, "+
				"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ=%.1f/ç§’, å¹³å‡=%.1f/ç§’\n",
				inserted, failed, batches, throughput, avgThroughput)

			lastInserted = inserted
		}
	}
}

// WideColumnOperations - ãƒ¯ã‚¤ãƒ‰ã‚«ãƒ©ãƒ æ“ä½œãƒ‡ãƒ¢
func (c *CassandraNoSQL) WideColumnOperations(ctx context.Context) {
	fmt.Println("\nğŸ“‹ Cassandra ãƒ¯ã‚¤ãƒ‰ã‚«ãƒ©ãƒ ãƒ‡ãƒ¢")
	fmt.Println("=" + repeatString("=", 50))

	if c.session == nil {
		fmt.Println("ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰: ãƒ¯ã‚¤ãƒ‰ã‚«ãƒ©ãƒ æ“ä½œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
		c.simulateWideColumns()
		return
	}

	// ãƒ¯ã‚¤ãƒ‰ãƒ­ãƒ¼ã®ä½œæˆ
	rowKey := fmt.Sprintf("user_%d", time.Now().Unix())

	// ä¸¦åˆ—ã§ã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
	var wg sync.WaitGroup
	numColumns := 1000

	for i := 0; i < numColumns; i++ {
		wg.Add(1)
		go func(colIndex int) {
			defer wg.Done()

			columnName := fmt.Sprintf("col_%d", colIndex)
			columnValue := []byte(fmt.Sprintf("value_%d_%d", colIndex, time.Now().Unix()))

			err := c.session.Query(`
				INSERT INTO async_practice.wide_rows
				(row_key, column_name, column_value, timestamp)
				VALUES (?, ?, ?, ?)`,
				rowKey, columnName, columnValue, time.Now(),
			).Exec()

			if err != nil && colIndex%100 == 0 {
				fmt.Printf("  ã‚«ãƒ©ãƒ æŒ¿å…¥ã‚¨ãƒ©ãƒ¼ %d: %v\n", colIndex, err)
			}
		}(i)
	}

	wg.Wait()
	fmt.Printf("  âœ“ %då€‹ã®ã‚«ãƒ©ãƒ ã‚’æŒ¿å…¥ã—ã¾ã—ãŸ (row_key: %s)\n", numColumns, rowKey)

	// ã‚¹ãƒ©ã‚¤ã‚¹ã‚¯ã‚¨ãƒª
	fmt.Println("\n  ã‚«ãƒ©ãƒ ã‚¹ãƒ©ã‚¤ã‚¹èª­ã¿å–ã‚Š:")
	iter := c.session.Query(`
		SELECT column_name, column_value
		FROM async_practice.wide_rows
		WHERE row_key = ?
		LIMIT 10`,
		rowKey,
	).Iter()

	var columnName string
	var columnValue []byte
	count := 0
	for iter.Scan(&columnName, &columnValue) {
		fmt.Printf("    %s: %s\n", columnName, string(columnValue))
		count++
	}

	if err := iter.Close(); err != nil {
		fmt.Printf("  èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: %v\n", err)
	} else {
		fmt.Printf("  âœ“ %då€‹ã®ã‚«ãƒ©ãƒ ã‚’èª­ã¿å–ã‚Šã¾ã—ãŸ\n", count)
	}
}

// CounterOperations - ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ“ä½œãƒ‡ãƒ¢
func (c *CassandraNoSQL) CounterOperations(ctx context.Context) {
	fmt.Println("\nğŸ”¢ Cassandra ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒ‡ãƒ¢")
	fmt.Println("=" + repeatString("=", 50))

	if c.session == nil {
		fmt.Println("ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰: ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ“ä½œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
		c.simulateCounters()
		return
	}

	counterID := fmt.Sprintf("counter_%d", time.Now().Unix())

	// ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®ä¸¦åˆ—æ›´æ–°
	var wg sync.WaitGroup
	numWorkers := 10
	incrementsPerWorker := 100

	fmt.Printf("  %då€‹ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ä¸¦åˆ—ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ›´æ–°...\n", numWorkers)

	for w := 0; w < numWorkers; w++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()

			for i := 0; i < incrementsPerWorker; i++ {
				err := c.session.Query(`
					UPDATE async_practice.counters
					SET count = count + ?
					WHERE counter_id = ?`,
					1, counterID,
				).Exec()

				if err != nil && i == 0 {
					fmt.Printf("    Worker %d: ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ›´æ–°ã‚¨ãƒ©ãƒ¼: %v\n", workerID, err)
				}
			}
		}(w)
	}

	wg.Wait()

	// ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼å€¤ã®èª­ã¿å–ã‚Š
	var count int64
	err := c.session.Query(`
		SELECT count FROM async_practice.counters
		WHERE counter_id = ?`,
		counterID,
	).Scan(&count)

	if err != nil {
		fmt.Printf("  ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: %v\n", err)
	} else {
		expected := numWorkers * incrementsPerWorker
		fmt.Printf("  âœ“ ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼å€¤: %d (æœŸå¾…å€¤: %d)\n", count, expected)
	}
}

// runDemoMode - ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ
func (c *CassandraNoSQL) runDemoMode(ctx context.Context) {
	fmt.Println("\nğŸ­ ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰: Cassandraã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")

	// ä»®æƒ³çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢
	type VirtualCassandra struct {
		mu    sync.RWMutex
		data  map[string]map[string]interface{}
		count int64
	}

	vc := &VirtualCassandra{
		data: make(map[string]map[string]interface{}),
	}

	// ä¸¦åˆ—æ›¸ãè¾¼ã¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
	var wg sync.WaitGroup
	for i := 0; i < 5; i++ {
		wg.Add(1)
		go func(id int) {
			defer wg.Done()
			for j := 0; j < 100; j++ {
				vc.mu.Lock()
				key := fmt.Sprintf("key_%d_%d", id, j)
				vc.data[key] = map[string]interface{}{
					"value":     j,
					"timestamp": time.Now(),
				}
				atomic.AddInt64(&vc.count, 1)
				vc.mu.Unlock()

				if j%20 == 0 {
					fmt.Printf("  Worker %d: %dä»¶å‡¦ç†\n", id, j)
				}
			}
		}(i)
	}

	wg.Wait()
	fmt.Printf("\n  âœ“ ãƒ‡ãƒ¢å®Œäº†: %dä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†\n", atomic.LoadInt64(&vc.count))
}

// simulateWideColumns - ãƒ¯ã‚¤ãƒ‰ã‚«ãƒ©ãƒ ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
func (c *CassandraNoSQL) simulateWideColumns() {
	row := make(map[string]interface{})
	var mu sync.Mutex

	// ä¸¦åˆ—ã‚«ãƒ©ãƒ è¿½åŠ 
	var wg sync.WaitGroup
	for i := 0; i < 100; i++ {
		wg.Add(1)
		go func(col int) {
			defer wg.Done()
			mu.Lock()
			row[fmt.Sprintf("col_%d", col)] = fmt.Sprintf("value_%d", col)
			mu.Unlock()
		}(i)
	}
	wg.Wait()

	fmt.Printf("  âœ“ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: %då€‹ã®ã‚«ãƒ©ãƒ ã‚’æŒã¤ãƒ¯ã‚¤ãƒ‰ãƒ­ãƒ¼\n", len(row))
}

// simulateCounters - ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
func (c *CassandraNoSQL) simulateCounters() {
	var counter int64

	// ä¸¦åˆ—ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ
	var wg sync.WaitGroup
	for i := 0; i < 10; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for j := 0; j < 100; j++ {
				atomic.AddInt64(&counter, 1)
			}
		}()
	}
	wg.Wait()

	fmt.Printf("  âœ“ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼å€¤ = %d\n", counter)
}

// Close - ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
func (c *CassandraNoSQL) Close() {
	if c.session != nil {
		c.session.Close()
	}
}